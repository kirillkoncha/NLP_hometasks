{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Русский текст: Вишневский стал довольным. Прекрасное далёко будь широким \\\n",
    "Мороженое мясо разморозили. Мороженное в ГУМе вкусное. НКРЯ — хороший корпус. Пила \\\n",
    "выглядела острой, Лена пила компот из сухофруктов. Ванная — помещение в квартире. \\\n",
    "Ванная комната. Глокой куздры развихры кудрячились по белесе. Спасло чудо техники. \\\n",
    "Спасли чудом техники. Ток не бей по лицу, пожалуйста. Ток ударил электрика. Бывший КГБшник, \\\n",
    "нынешний президент. Здравствуйте, дорогой Мартин Алексеич! Дорогая, пойдем уже отсюда? \\\n",
    "Спел песню. Посмотри, как спел этот фрукт. Смешное животное смешно смеялось. \\\n",
    "Мне кажется, уже достаточно много трудных для POS теггинга случаев и \\\n",
    "я не знаю, что дальше писать и откуда брать текст, но до ста слов дойти хочется.'\n",
    "\n",
    "Неоднозначные моменты для тэггинга: Вишневский -- ADJ или NOUN? Далёко -- как существительное и наречие; Мороженое мясо и мороженое -- конверсивы; Острая пила и Лена пила сок -- омонимы; Ванная как прилигательное и существительное; Ток -- как частица только и ток как электрический ток; Дорогой Мартин Алексеич и Дорогая -- конверсивы; Спел фрукт и спел песню -- омонимы; POS -- английская вставка. Также есть аббревиатуры (ГУМ, НКРЯ) и выдуманные слова, имеющие морфологию частей речи (например глокая -- прил,куздра сущ и т.д.)\n",
    "\n",
    "Английский текст: Dr. Young healed young child. Lying on the bed is good after a hard day. \\\n",
    "He is lying. White people and black people. This sentence is put here for more words \\\n",
    "and diversity. Mr. White works at the school. He walked cat for a catwalk. He lights \\\n",
    "the lanterns. Lights were bright in the dark. POS tagging is quite interesting thing. \\\n",
    "Walking in the rain is my hobby. He was walking in the rain. I play my music. \\\n",
    "Russian indie bands are super. It is raining. Streets are full of different people. \\\n",
    "NRA has their people in the senate of the US.\n",
    "\n",
    "Неоднозначные моменты: конверсивы в собственных именах и прилагательных (Dr. Young и young, Mr. White); омонимы (lying как глагол и существительное; lights как форма глагола и существительное; walking как форма глагола и существительное); Аббревиуатуры и имена собственные (NRA, US).\n",
    "\n",
    "Разметку текстов можно найти в функциях проверки accuracy ниже. Сразу отмечу, что к такому стандарту я и привожу output'ы разных тэггеров.\n",
    "\n",
    "Самым эффективным русским тэггором оказался mystem, а английским -- nltk. К слову, flair слова с дефисом за два слова считает, поэтому в английском варианте текста \"POS-tagging\" пришлось превратить в \"POS tagging\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kirillkonca1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-15 01:00:05,444 loading file /Users/kirillkonca1/.flair/models/pos-multi-v0.1.pt\n"
     ]
    }
   ],
   "source": [
    "import navec\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from navec import Navec\n",
    "from slovnet import NER\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ipymarkup import show_span_ascii_markup as show_markup\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger, \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "    Doc\n",
    ")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "segmenter = Segmenter()\n",
    "morph = MorphAnalyzer()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "m = Mystem()\n",
    "tagger = SequenceTagger.load('pos-multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Вишневский стал довольным. Прекрасное далёко будь широким \\\n",
    "Мороженое мясо разморозили. Мороженное в ГУМе вкусное. НКРЯ — хороший корпус. Пила \\\n",
    "выглядела острой, Лена пила компот из сухофруктов. Ванная — помещение в квартире. \\\n",
    "Ванная комната. Глокой куздры развихры кудрячились по белесе. Спасло чудо техники. \\\n",
    "Спасли чудом техники. Ток не бей по лицу, пожалуйста. Ток ударил электрика. Бывший КГБшник, \\\n",
    "нынешний президент. Здравствуйте, дорогой Мартин Алексеич! Дорогая, пойдем уже отсюда? \\\n",
    "Спел песню. Посмотри, как спел этот фрукт. Смешное животное смешно смеялось. \\\n",
    "Мне кажется, уже достаточно много трудных для POS теггинга случаев и \\\n",
    "я не знаю, что дальше писать и откуда брать текст, но до ста слов дойти хочется.'\n",
    "\n",
    "text_eng = 'Dr. Young healed young child. Lying on the bed is good after a hard day. \\\n",
    "He is lying. White people and black people. This sentence is put here for more words \\\n",
    "and diversity. Mr. White works at the school. He walked cat for a catwalk. He lights \\\n",
    "the lanterns. Lights were bright in the dark. POS tagging is quite interesting thing. \\\n",
    "Walking in the rain is my hobby. He was walking in the rain. I play my music. \\\n",
    "Russian indie bands are super. It is raining. Streets are full of different people. \\\n",
    "NRA has their people in the senate of the US.'\n",
    "\n",
    "\n",
    "def pymorph(text):\n",
    "    tags = []\n",
    "    for i in text.split():\n",
    "        if i != '—':\n",
    "            p = morph.parse(i.translate(str.maketrans('', '', string.punctuation)))[0]\n",
    "            if p.tag.POS == 'ADJF':\n",
    "                tags.append('ADJ')\n",
    "            elif p.tag.POS == 'ADJS':\n",
    "                tags.append('ADJ')\n",
    "            elif p.tag.POS == 'COMP':\n",
    "                tags.append('ADV')\n",
    "            elif p.tag.POS == 'INFN':\n",
    "                tags.append('VERB')\n",
    "            elif p.tag.POS == 'PRTF':\n",
    "                tags.append('VERB')\n",
    "            elif p.tag.POS == 'GRND':\n",
    "                tags.append('VERB')\n",
    "            elif p.tag.POS == 'PRTS':\n",
    "                tags.append('VERB')\n",
    "            elif p.tag.POS == 'PRCL':\n",
    "                tags.append('PART')\n",
    "            elif p.tag.POS == 'PRED':\n",
    "                tags.append('ADV')\n",
    "            elif p.tag.POS == 'ADVB':\n",
    "                tags.append('ADV')\n",
    "            elif p.tag.POS == 'NPRO':\n",
    "                tags.append('PRO')\n",
    "            elif p.tag.POS == None:\n",
    "                tags.append('None')\n",
    "            else:\n",
    "                tags.append(p.tag.POS)\n",
    "    return tags\n",
    "\n",
    "def mystem(text):\n",
    "    ana = m.analyze(text)\n",
    "    tags = []\n",
    "    for word in ana:\n",
    "        try:\n",
    "            if 'analysis' in word:\n",
    "                gr = word['analysis'][0]['gr']\n",
    "                pos = gr.split('=')[0].split(',')[0]\n",
    "                if pos == 'S':\n",
    "                    tags.append('NOUN')\n",
    "                elif pos == 'V':\n",
    "                    tags.append('VERB')\n",
    "                elif pos == 'A':\n",
    "                    tags.append('ADJ')\n",
    "                elif pos == 'ADVPRO':\n",
    "                    tags.append('ADV')\n",
    "                elif pos == 'NUM':\n",
    "                    tags.append('NUMR')\n",
    "                elif pos == 'SPRO':\n",
    "                    tags.append('PRO') \n",
    "                elif pos == 'PR':\n",
    "                    tags.append('PREP') \n",
    "                elif pos == 'APRO':\n",
    "                    tags.append('ADJ')\n",
    "                elif pos == 'ANUM':\n",
    "                    tags.append('ADJ')\n",
    "                else:\n",
    "                    tags.append(pos)\n",
    "        except IndexError: #except потому что при парсинге английского POS крашится, так как там analysis пустой\n",
    "            tags.append('None')\n",
    "    return tags\n",
    "\n",
    "        \n",
    "def nat(text):\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    tags = []\n",
    "    for i in doc.tokens:\n",
    "        if i.pos != 'PUNCT':\n",
    "            if i.pos == 'PROPN':\n",
    "                tags.append('NOUN')\n",
    "            elif i.pos == 'AUX':\n",
    "                tags.append('VERB')\n",
    "            elif i.pos == 'SCONJ':\n",
    "                tags.append('CONJ')\n",
    "            elif i.pos == 'CCONJ':\n",
    "                tags.append('CONJ')\n",
    "            elif i.pos == 'DET':\n",
    "                tags.append('PRO')\n",
    "            elif i.pos == 'ADP':\n",
    "                tags.append('PREP')\n",
    "            else:\n",
    "                tags.append(i.pos)\n",
    "    return tags\n",
    "\n",
    "def spacy_tag(text):\n",
    "    tags = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ != 'PUNCT':\n",
    "            if token.pos_ == 'PROPN':\n",
    "                tags.append('NOUN')\n",
    "            elif token.pos_ == 'ADP':\n",
    "                tags.append('PREP')\n",
    "            elif token.pos_ == 'CCONJ':\n",
    "                tags.append('CONJ')\n",
    "            elif token.pos_ == 'AUX':\n",
    "                tags.append('VERB')\n",
    "            else:\n",
    "                tags.append(token.pos_)           \n",
    "    return tags\n",
    "\n",
    "def flair_tag(text):\n",
    "    tags = []\n",
    "    sentence = Sentence(text)\n",
    "    tagger.predict(sentence)\n",
    "    tags_string = sentence.to_tagged_string()\n",
    "    search = re.findall('\\<\\w+\\>', tags_string)\n",
    "    for tag in search:\n",
    "        tag_n = re.sub('[^A-Za-z]+', '', tag)\n",
    "        if tag_n != 'PUNCT':\n",
    "            if tag_n == 'PROPN':\n",
    "                tags.append('NOUN')\n",
    "            elif tag_n == 'AUX':\n",
    "                tags.append('VERB')\n",
    "            elif tag_n == 'ADP':\n",
    "                tags.append('PREP')\n",
    "            elif tag_n == 'CCONJ':\n",
    "                tags.append('CONJ')\n",
    "            elif tag_n == 'PRO':\n",
    "                tags.append('PRON')\n",
    "            else:\n",
    "                tags.append(tag_n)\n",
    "    return tags\n",
    "\n",
    "def nltk_tags(text):\n",
    "    tags = []\n",
    "    tokenize = word_tokenize(text)\n",
    "    nltk.pos_tag(tokenize)\n",
    "    for i in nltk.pos_tag(tokenize):\n",
    "        if i[1] != '.':\n",
    "            if i[1] == 'CC':\n",
    "                tags.append('CONJ')\n",
    "            elif i[1] == 'DT':\n",
    "                tags.append('DET')\n",
    "            elif i[1] == 'NN' or i[1] == 'NNS' or i[1] == 'NNPS' or i[1] == 'NNP':\n",
    "                tags.append('NOUN')\n",
    "            elif i[1] == 'JJ' or i[1] == 'JJR' or i[1] == 'JJS':\n",
    "                tags.append('ADJ')\n",
    "            elif i[1] == 'IN':\n",
    "                tags.append('PREP')\n",
    "            elif i[1] == 'PRP' or i[1] == 'PRP$':\n",
    "                tags.append('PRO')\n",
    "            elif i[1] == 'RB':\n",
    "                tags.append('ADV')\n",
    "            elif i[1] == 'VB' or i[1] == 'VBD' or i[1] == 'VBG' \\\n",
    "            or i[1] == 'VBN' or i[1] == 'VBP' or i[1] == 'VBZ':\n",
    "                tags.append('VERB')\n",
    "    return tags\n",
    "\n",
    "def accuracy_check_rus(tags):\n",
    "    gold = ['NOUN', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'ADJ',\n",
    "            'ADJ', 'NOUN', 'VERB', 'NOUN', 'PREP', 'NOUN', 'ADJ', 'NOUN', \n",
    "            'ADJ', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'VERB', 'NOUN',\n",
    "            'PREP', 'NOUN', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'ADJ', 'NOUN',\n",
    "            'ADJ', 'NOUN', 'NOUN', 'VERB', 'PREP', 'NOUN', 'VERB', 'NOUN',\n",
    "            'NOUN', 'VERB', 'NOUN', 'NOUN', 'PART', 'PART', 'VERB', 'PREP',\n",
    "            'NOUN', 'PART', 'NOUN', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'ADJ',\n",
    "            'NOUN', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'ADJ', 'VERB', 'PART',\n",
    "            'ADV', 'VERB', 'NOUN', 'VERB', 'PART', 'ADJ', 'PRO', 'NOUN',\n",
    "            'ADJ', 'NOUN', 'ADV', 'VERB', 'PRO', 'VERB', 'PART', 'ADJ',\n",
    "            'NUM', 'ADJ', 'PREP', 'NOUN', 'NOUN', 'NOUN', 'CONJ', 'PRO',\n",
    "            'PRCL', 'VERB', 'CONJ', 'ADV', 'VERB', 'CONJ', 'ADV', 'VERB',\n",
    "            'NOUN', 'CONJ', 'PREP', 'NUMR', 'NOUN', 'VERB', 'VERB']\n",
    "    print(\"Accuracy: %.4f\" % accuracy_score(tags, gold))\n",
    "\n",
    "def accuracy_check_eng(tags):\n",
    "    gold = ['NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'PREP', 'DET', 'NOUN', 'VERB', 'ADJ', 'PREP', \n",
    "            'DET', 'ADJ', 'NOUN', 'PRO', 'VERB', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'ADJ', 'NOUN',\n",
    "            'PRO', 'NOUN', 'VERB', 'VERB', 'ADV', 'PREP', 'ADJ', 'NOUN', 'CONJ', 'NOUN', 'NOUN',\n",
    "            'NOUN', 'VERB', 'PREP', 'DET', 'NOUN', 'PRO', 'VERB', 'NOUN', 'PREP', 'DET', 'NOUN',\n",
    "            'PRO', 'VERB', 'DET', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'PREP', 'DET', 'NOUN', 'NOUN',\n",
    "            'NOUN', 'VERB', 'ADV', 'ADJ', 'NOUN', 'NOUN', 'PREP', 'DET', 'NOUN', 'VERB', 'PRO', 'NOUN',\n",
    "            'PRO', 'VERB', 'VERB', 'PREP', 'DET', 'NOUN', 'PRO', 'VERB', 'PRO', 'NOUN', 'ADJ',\n",
    "            'ADJ', 'NOUN', 'VERB', 'ADJ', 'PRO', 'VERB', 'VERB', 'NOUN', 'VERB', 'ADJ', 'PREP', 'ADJ',\n",
    "            'NOUN', 'NOUN', 'VERB', 'PRO', 'NOUN', 'PREP', 'DET', 'NOUN', 'PREP', 'DET', 'NOUN']\n",
    "    print(\"Accuracy: %.4f\" % accuracy_score(tags, gold))\n",
    "    \n",
    "def adj_noun(text):\n",
    "    ana = m.analyze(text)\n",
    "    adj = ''\n",
    "    for word in ana:\n",
    "        if 'analysis' in word:\n",
    "            gr = word['analysis'][0]['gr']\n",
    "            pos = gr.split('=')[0].split(',')[0]\n",
    "            lex = word['text']\n",
    "            if adj == '':\n",
    "                if pos == 'A':\n",
    "                    adj = lex\n",
    "            else:\n",
    "                if pos == 'S':\n",
    "                    print(adj, '', lex)\n",
    "                    adj = ''\n",
    "                elif pos!= 'A':\n",
    "                    adj = ''\n",
    "                else:\n",
    "                    adj = lex\n",
    "\n",
    "def ne_verb(text):\n",
    "    ana = m.analyze(text)\n",
    "    ne = ''\n",
    "    for word in ana:\n",
    "        if 'analysis' in word:\n",
    "            gr = word['analysis'][0]['gr']\n",
    "            pos = gr.split('=')[0].split(',')[0]\n",
    "            lex = word['text']\n",
    "            if ne == '':\n",
    "                if lex == 'не':\n",
    "                    ne = lex\n",
    "            else:\n",
    "                if pos == 'V':\n",
    "                    print(ne, '', lex)\n",
    "                    ne = ''\n",
    "                elif lex!= 'не':\n",
    "                    ne = ''\n",
    "                else:\n",
    "                    ne = lex\n",
    "\n",
    "def noun_adj(text):\n",
    "    ana = m.analyze(text)\n",
    "    noun = ''\n",
    "    for word in ana:\n",
    "        if 'analysis' in word:\n",
    "            gr = word['analysis'][0]['gr']\n",
    "            pos = gr.split('=')[0].split(',')[0]\n",
    "            lex = word['text']\n",
    "            if noun == '':\n",
    "                if pos == 'S':\n",
    "                    noun = lex\n",
    "            else:\n",
    "                if pos == 'A':\n",
    "                    print(noun, '', lex)\n",
    "                    ne = ''\n",
    "                elif lex!= 'S':\n",
    "                    noun = ''\n",
    "                else:\n",
    "                    noun = lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8529\n"
     ]
    }
   ],
   "source": [
    "accuracy_check_rus(mystem(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8137\n"
     ]
    }
   ],
   "source": [
    "accuracy_check_rus(nat(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7941\n"
     ]
    }
   ],
   "source": [
    "accuracy_check_rus(pymorph(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'VERB', 'ADJ', 'ADJ', 'ADJ', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'PREP', 'NOUN', 'ADJ', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'PREP', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'VERB', 'ADV', 'NOUN', 'NOUN', 'PART', 'VERB', 'PREP', 'NOUN', 'PART', 'NOUN', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'ADV', 'NOUN', 'NOUN', 'ADJ', 'VERB', 'ADV', 'ADV', 'VERB', 'NOUN', 'VERB', 'CONJ', 'VERB', 'ADJ', 'NOUN', 'ADJ', 'ADJ', 'ADV', 'VERB', 'PRO', 'CONJ', 'ADV', 'ADV', 'ADV', 'ADJ', 'PREP', 'None', 'NOUN', 'NOUN', 'CONJ', 'PRO', 'PART', 'VERB', 'CONJ', 'ADV', 'VERB', 'CONJ', 'ADV', 'VERB', 'NOUN', 'CONJ', 'PREP', 'NUMR', 'NOUN', 'VERB', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "print(pymorph(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'VERB', 'ADJ', 'ADJ', 'ADV', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'PREP', 'NOUN', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'PREP', 'NOUN', 'ADJ', 'NOUN', 'PREP', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'PREP', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'PART', 'VERB', 'PREP', 'NOUN', 'PART', 'NOUN', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'ADJ', 'VERB', 'ADV', 'ADV', 'VERB', 'NOUN', 'VERB', 'CONJ', 'VERB', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'ADV', 'VERB', 'PRO', 'VERB', 'ADV', 'ADV', 'ADV', 'ADJ', 'PREP', 'None', 'NOUN', 'NOUN', 'CONJ', 'PRO', 'PART', 'VERB', 'CONJ', 'ADV', 'VERB', 'CONJ', 'ADV', 'VERB', 'NOUN', 'CONJ', 'PREP', 'NUMR', 'NOUN', 'VERB', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "print(mystem(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'PREP', 'NOUN', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'X', 'ADJ', 'NOUN', 'PREP', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'PART', 'VERB', 'PREP', 'NOUN', 'ADV', 'NOUN', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'ADJ', 'VERB', 'ADV', 'ADV', 'VERB', 'NOUN', 'VERB', 'CONJ', 'VERB', 'PRO', 'NOUN', 'ADJ', 'NOUN', 'ADJ', 'ADJ', 'PRON', 'VERB', 'ADV', 'ADV', 'ADV', 'ADJ', 'PREP', 'NOUN', 'ADJ', 'NOUN', 'CONJ', 'PRON', 'PART', 'VERB', 'CONJ', 'ADV', 'VERB', 'CONJ', 'ADV', 'VERB', 'NOUN', 'CONJ', 'PREP', 'NUM', 'NOUN', 'VERB', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "print(nat(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'VERB', 'PREP', 'DET', 'NOUN', 'VERB', 'ADJ', 'PREP', 'DET', 'ADJ', 'NOUN', 'PRON', 'VERB', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'ADJ', 'NOUN', 'DET', 'NOUN', 'VERB', 'VERB', 'ADV', 'PREP', 'ADJ', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'PREP', 'DET', 'NOUN', 'PRON', 'VERB', 'NOUN', 'PREP', 'DET', 'NOUN', 'PRON', 'VERB', 'DET', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'PREP', 'DET', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'ADV', 'ADJ', 'NOUN', 'VERB', 'PREP', 'DET', 'NOUN', 'VERB', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', 'PREP', 'DET', 'NOUN', 'PRON', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'PRON', 'VERB', 'VERB', 'NOUN', 'VERB', 'ADJ', 'PREP', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'DET', 'NOUN', 'PREP', 'DET', 'NOUN', 'PREP', 'DET', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "print(spacy_tag(text_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'VERB', 'PREP', 'DET', 'NOUN', 'VERB', 'ADJ', 'PREP', 'DET', 'ADJ', 'NOUN', 'PRON', 'VERB', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'ADJ', 'NOUN', 'DET', 'NOUN', 'VERB', 'VERB', 'ADV', 'PREP', 'ADJ', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'PREP', 'DET', 'NOUN', 'PRON', 'VERB', 'NOUN', 'PREP', 'DET', 'NOUN', 'PRON', 'VERB', 'DET', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'PREP', 'DET', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'ADV', 'ADJ', 'NOUN', 'VERB', 'PREP', 'DET', 'NOUN', 'VERB', 'PRON', 'NOUN', 'PRON', 'VERB', 'VERB', 'PREP', 'DET', 'NOUN', 'PRON', 'VERB', 'PRON', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'PRON', 'VERB', 'VERB', 'NOUN', 'VERB', 'ADJ', 'PREP', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'PRON', 'NOUN', 'PREP', 'DET', 'NOUN', 'PREP', 'DET', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "print(flair_tag(text_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'VERB', 'PREP', 'DET', 'NOUN', 'VERB', 'ADJ', 'PREP', 'DET', 'ADJ', 'NOUN', 'PRO', 'VERB', 'VERB', 'NOUN', 'NOUN', 'CONJ', 'ADJ', 'NOUN', 'DET', 'NOUN', 'VERB', 'VERB', 'ADV', 'PREP', 'ADJ', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'PREP', 'DET', 'NOUN', 'PRO', 'VERB', 'NOUN', 'PREP', 'DET', 'NOUN', 'PRO', 'VERB', 'DET', 'NOUN', 'NOUN', 'VERB', 'VERB', 'PREP', 'DET', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'ADV', 'ADJ', 'NOUN', 'VERB', 'PREP', 'DET', 'NOUN', 'VERB', 'PRO', 'NOUN', 'PRO', 'VERB', 'VERB', 'PREP', 'DET', 'NOUN', 'PRO', 'VERB', 'PRO', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'PRO', 'VERB', 'VERB', 'NOUN', 'VERB', 'ADJ', 'PREP', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'PRO', 'NOUN', 'PREP', 'DET', 'NOUN', 'PREP', 'DET', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "print(nltk_tags(text_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8725\n"
     ]
    }
   ],
   "source": [
    "accuracy_check_eng(spacy_tag(text_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8725\n"
     ]
    }
   ],
   "source": [
    "accuracy_check_eng(flair_tag(text_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9412\n"
     ]
    }
   ],
   "source": [
    "accuracy_check_eng(nltk_tags(text_eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы улучшить работы программы из предыдущей дз, предлагаю выделить следующие синтаксические группы: сущ + прил и прил + сущ, т.к достаточно часто в зависимости от рецензии сторонам фильма даётся характеристика: плохая режиссура, саундтрек великолепен и т.д. Для только отрицательных рецензий можно выделить не + глаг., так как при описании впечатлений автор может перечислять то, что он ожидал от фильма, но ему не удалось реализовать: не впечатлил, не заинтересовал, не понравился и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = 'Ремейк «Мулан» от Дисней — явление само по себе интересное. Прошлые реинкарнации \\\n",
    "анимационного наследия студии — вроде фавровских «Короля льва» с  \\\n",
    "«Книгой джунглей» и «Аладдина» Гая Ричи — продвигали в первую очередь как старые \\\n",
    "истории на новый лад, как способ ещё раз, но в другой форме, пережить «тот самый» \\\n",
    "детский экспириенс. С «Мулан» всё несколько сложнее: видите ли, оригинальный мультфильм, \\\n",
    "рассказывающий о народной китайской героине, понравился примерно всем, кроме самих китайцев. \\\n",
    "Потому что и дракон Мушу — персонаж оскорбительный (негоже величественным драконам быть смехотворными), \\\n",
    "и сама Мулан (которая вообще-то Хуа Мулань) вдруг стала индивидуалисткой-феминисткой, а не \\\n",
    "преданной защитницей Китайской Империи и китайской национальной идеи. В 1998-м недовольство \\\n",
    "одной отдельной закрытой страны, допустим, легко можно было пережить. В 2020-м продюсерам \\\n",
    "Дисней легче вынести отсутствие домашнего проката, чем невыпуск кино в Китае. \\\n",
    "Поэтому и основной опорой в маркетинге «Мулан» была не эксплуатация ностальгии, а, наоборот, \\\n",
    "всяческие увещевания, что экранизация совершенно не будет походить на мультфильм. Что она будет \\\n",
    "более приземлённой, историчной и суровой — якобы поэтому и Мушу, персонажу явно фэнтезийному, места \\\n",
    "в ней не нашлось. Реальность оказалась несколько более прозаичной: ни о какой приземлённости и \\\n",
    "историчности в новой «Мулан», как и в оригинальном мультфильме, речи не идёт — не стала свежая \\\n",
    "экранизация и ближе к изначальной легенде о Хуа Мулань. Магии здесь стало даже больше: есть и \\\n",
    "ведьмы, и фениксы, да и силы главной героини объясняются не упорством и трудом, а предрасположенностью \\\n",
    "к этакой Силе из «Звёздных войн» (точнее, взаимосвязь тут обратная, но неважно). \\\n",
    "Если вы ищете приземлённую историю героини — так её уже сняли сами китайцы в 2009, \\\n",
    "Хуа Мулан называется. А это просто ещё одна сказка, только со слегка сбившейся в \\\n",
    "сторону коммунизма оптикой.'\n",
    "                \n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Прошлые  реинкарнации\n",
      "анимационного  наследия\n",
      "старые  истории\n",
      "новый  лад\n",
      "детский  экспириенс\n",
      "оригинальный  мультфильм\n",
      "китайской  героине\n",
      "величественным  драконам\n",
      "преданной  защитницей\n",
      "Китайской  Империи\n",
      "национальной  идеи\n",
      "домашнего  проката\n",
      "основной  опорой\n",
      "всяческие  увещевания\n",
      "фэнтезийному  места\n",
      "новой  Мулан\n",
      "оригинальном  мультфильме\n",
      "свежая  экранизация\n",
      "изначальной  легенде\n",
      "главной  героини\n",
      "Звёздных  войн\n",
      "приземлённую  историю\n"
     ]
    }
   ],
   "source": [
    "adj_noun(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "реинкарнации  анимационного\n",
      "персонаж  оскорбительный\n",
      "защитницей  Китайской\n",
      "отсутствие  домашнего\n",
      "силы  главной\n"
     ]
    }
   ],
   "source": [
    "noun_adj(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не  будет\n",
      "не  нашлось\n",
      "не  идёт\n",
      "не  стала\n"
     ]
    }
   ],
   "source": [
    "ne_verb(review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
